{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishnaugale353/TB_detection/blob/main/tb_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fUWaHzfzcr1-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import math\n",
        "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from collections.abc import Iterable\n",
        "import glob"
      ],
      "metadata": {
        "id": "ombbKUmfesdu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=30\n",
        "patch_size=16\n",
        "img_size=224\n",
        "num_patches=(img_size//patch_size)**2\n",
        "p_dim=768\n",
        "heads_att=12\n",
        "num_encoder=12"
      ],
      "metadata": {
        "id": "2TbWqiBDe4n7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_class={0:\"Normal\",1:\"Tuberculosis\"}\n",
        "Normal_dir=Path(\"/content/drive/MyDrive/archive/TB_Chest_Radiography_Database/Normal\")\n",
        "TB_dir=Path(\"/content/drive/MyDrive/archive/TB_Chest_Radiography_Database/Tuberculosis\")"
      ],
      "metadata": {
        "id": "glC9bjUfe7VY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Normal_images:list=sorted(list(map(str, list(Normal_dir.glob(\"*.png\")))))\n",
        "TB_images:list=sorted(list(map(str, list(TB_dir.glob(\"*.png\")))))\n",
        "Normal_labels:list=[0]*len(Normal_images)\n",
        "TB_labels:list=[1]*len(TB_images)"
      ],
      "metadata": {
        "id": "iK1t1XG5fUXM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images=np.array(Normal_images+TB_images)\n",
        "labels=np.array(Normal_labels+TB_labels)\n",
        "images.shape, labels.shape"
      ],
      "metadata": {
        "id": "M1zvEvG8fYOm",
        "outputId": "15932823-51e3-4044-caf6-aa968cb0f168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0,), (0,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(images,labels,test_size=0.2,random_state=42)\n",
        "x_valid,x_test,y_valid,y_test=train_test_split(x_valid,y_valid,test_size=0.5,random_state=42)\n",
        "(x_train.shape,x_valid.shape,x_test.shape)"
      ],
      "metadata": {
        "id": "v8N3cNavfvRd",
        "outputId": "0212cffe-a620-47e7-8493-c0bb57d95879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c4b6af1ae04c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocessing(path):\n",
        "    img=cv2.imread(path)\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    clahe=cv2.createCLAHE(clipLimit=2)\n",
        "    img=clahe.apply(img)\n",
        "    img=cv2.GaussianBlur(img,(5,5),0,borderType=cv2.BORDER_CONSTANT)\n",
        "    img=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_LINEAR)\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "    img=np.moveaxis(img,-1,0)\n",
        "    return img"
      ],
      "metadata": {
        "id": "fGtV2KyXf4Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "VvdrocKGgTR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(list(map(image_preprocessing,x_train))).astype(np.float32)\n",
        "x_valid=np.array(list(map(image_preprocessing,x_valid))).astype(np.float32)\n",
        "x_test=np.array(list(map(image_preprocessing,x_test))).astype(np.float32)\n",
        "x_train.shape,x_valid.shape,x_test.shape"
      ],
      "metadata": {
        "id": "uY8hBK4Af7Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x_train=torch.from_numpy(x_train).to(device)\n",
        "x_valid=torch.from_numpy(x_valid).to(device)\n",
        "x_test=torch.from_numpy(x_test).to(device)\n",
        "y_train=torch.from_numpy(y_train).to(device)\n",
        "y_valid=torch.from_numpy(y_valid).to(device)\n",
        "y_test=torch.from_numpy(y_test).to(device)"
      ],
      "metadata": {
        "id": "fvpw7Hn9f-eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardScaler():\n",
        "    def __init__(self) -> None:\n",
        "        self.mean=None\n",
        "        self.std=None\n",
        "    def fit(self,tensor:torch.Tensor) -> None:\n",
        "        self.mean=tensor.mean((0,2,3),keepdim=True)\n",
        "        self.std=tensor.std((0,2,3),keepdim=True)\n",
        "    def transform(self,tensor:torch.Tensor) -> torch.Tensor:\n",
        "        scaled=(tensor-self.mean)/(self.std+1e-5)\n",
        "        return scaled\n",
        "    def fit_transform(self,tensor:torch.Tensor) -> torch.Tensor:\n",
        "        self.fit(tensor=tensor)\n",
        "        scaled=self.transform(tensor=tensor)\n",
        "        return scaled"
      ],
      "metadata": {
        "id": "B4XaQd1HiWOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()\n",
        "x_train=scaler.fit_transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_valid=scaler.transform(x_valid)"
      ],
      "metadata": {
        "id": "wqJP0nariW8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "MK8Waa7EidD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=TensorDataset(x_train,y_train)\n",
        "val_dataset=TensorDataset(x_valid,y_valid)\n",
        "test_dataset=TensorDataset(x_test,y_test)\n",
        "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
        "val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
        "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "nx5T-YFxigOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_patches(x:torch.Tensor, kernel:int, stride:int=1, dilation:int=1):\n",
        "\n",
        "    b,c,h,w = x.shape\n",
        "    h2 = math.ceil(h / stride)\n",
        "    w2 = math.ceil(w / stride)\n",
        "    pad_row = (h2 - 1) * stride + (kernel - 1) * dilation + 1 - h\n",
        "    pad_col = (w2 - 1) * stride + (kernel - 1) * dilation + 1 - w\n",
        "    x = torch.nn.functional.pad(x, (pad_row//2, pad_row - pad_row//2, pad_col//2, pad_col - pad_col//2))\n",
        "\n",
        "    # Extract patches\n",
        "    patches = x.unfold(2, kernel, stride).unfold(3, kernel, stride)\n",
        "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "    patches = patches.view(*patches.size()[:3], -1)\n",
        "\n",
        "    #flatten patches\n",
        "    return patches.view(b,-1,patches.shape[-1])"
      ],
      "metadata": {
        "id": "jcpBkVaGii-Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(torch.nn.Module):\n",
        "    def __init__(self, projection_dim:int, num_heads:int, num_patches:int):\n",
        "        super().__init__()\n",
        "        self.projection_dim=projection_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.num_patches=num_patches\n",
        "\n",
        "        self.norm1=torch.nn.LayerNorm(self.projection_dim)\n",
        "        self.norm2=torch.nn.LayerNorm(self.projection_dim)\n",
        "        self.attention=torch.nn.MultiheadAttention(self.projection_dim,self.num_heads,batch_first=True)\n",
        "        self.mlp=torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.projection_dim,self.projection_dim*4),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(self.projection_dim*4,self.projection_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1=self.norm1(x)\n",
        "        attention=self.attention(x1,x1,x1)[0]\n",
        "        x2=attention+x\n",
        "        x3=self.norm2(x2)\n",
        "        x3=self.mlp(x3)\n",
        "        out=x2+x3\n",
        "        return out"
      ],
      "metadata": {
        "id": "CsGgXBCTim2K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PEG(torch.nn.Module):\n",
        "    def __init__(self,dim:int,k:int=3):\n",
        "        super().__init__()\n",
        "        self.pos=torch.nn.Conv2d(dim,dim,k,1,1,groups=dim)\n",
        "    def forward(self,x,H,W):\n",
        "        B,N,C=x.shape\n",
        "        feat_tokens=x\n",
        "        feat_tokens=feat_tokens.transpose(1,2).view(B,C,H,W)\n",
        "        x=self.pos(feat_tokens)+feat_tokens\n",
        "        x=x.flatten(2).transpose(1,2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2CP_JJAnirKI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 patch_size:int,\n",
        "                 num_patches:int,\n",
        "                 projection_dim:int,\n",
        "                 num_heads:int,\n",
        "                 num_encoder:int):\n",
        "        super().__init__()\n",
        "        self.patch_size=patch_size\n",
        "        self.num_patches=num_patches\n",
        "        self.projection_dim=projection_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.num_encoder=num_encoder\n",
        "\n",
        "        self.input_d=self.patch_size*self.patch_size*3\n",
        "        self.linear=torch.nn.Linear(self.input_d,self.projection_dim)\n",
        "        self.peg=PEG(self.projection_dim,3)\n",
        "        self.blocks = torch.nn.ModuleList([\n",
        "            EncoderBlock(self.projection_dim, self.num_heads, self.num_patches) for _ in range(self.num_encoder)\n",
        "        ])\n",
        "        self.ln_out=torch.nn.LayerNorm(self.projection_dim)\n",
        "\n",
        "        self.out=torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.projection_dim,1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,images:torch.Tensor):\n",
        "        batch_size=images.size()[0]\n",
        "        x=extract_image_patches(images,self.patch_size,self.patch_size)\n",
        "        x=self.linear(x)\n",
        "        encoded=x\n",
        "\n",
        "        for i,block in enumerate(self.blocks):\n",
        "            encoded=block(encoded)\n",
        "            if i==0:\n",
        "                _H,_W=int(self.num_patches**0.5),int(self.num_patches**0.5)\n",
        "                encoded=self.peg(encoded,_H,_W)\n",
        "\n",
        "        rep=encoded.mean(dim=1)\n",
        "        rep=self.ln_out(rep)\n",
        "        output=self.out(rep)\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZnuGBkSgiwa0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ViT(patch_size=patch_size,\n",
        "          num_patches=num_patches,\n",
        "          projection_dim=p_dim,\n",
        "          num_heads=heads_att,\n",
        "          num_encoder=num_encoder).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "OzOmBVaEi1ED",
        "outputId": "5cc4ae55-4a77-4c1d-d598-a7836787603d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bce30fd59697>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mprojection_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads_att\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           num_encoder=num_encoder).to(device)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_total_params = sum(p.numel() for p in model.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "x2fpCESSi4QD",
        "outputId": "7349cf9f-594a-4c9e-dfcc-55d54e4512b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c4ce122a3fb9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvit_total_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvit_total_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=30\n",
        "loss_fn=torch.nn.BCELoss()\n",
        "opt=torch.optim.SGD(model.parameters(),lr=1e-3,momentum=0.9)\n",
        "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt,T_max=epoch,verbose=True)"
      ],
      "metadata": {
        "id": "h6dcs-wfjOES",
        "outputId": "3820a61a-2ff4-4ed1-bd04-53a3a51188a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f7111accdb8c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float('inf')\n",
        "best_model_weights = None\n",
        "patience=10\n",
        "for i in range(epoch):\n",
        "    print(f\"Epoch: {i+1}\\n\")\n",
        "    model.train()\n",
        "    train_loss=0\n",
        "    train_batches=len(train_loader)\n",
        "    for batch,(x,y) in enumerate(train_loader):\n",
        "        pred=model(x).squeeze(-1)\n",
        "        loss=loss_fn(pred,y.float())\n",
        "        train_loss+=loss.item()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    train_loss/=train_batches\n",
        "    print(f\"Train Error: \\n Avg loss: {train_loss:>8f} \\n\")\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    test_batches=len(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for x,y in val_loader:\n",
        "            pred=model(x).squeeze(-1)\n",
        "            test_loss += loss_fn(pred,y.float()).item()\n",
        "    test_loss /= test_batches\n",
        "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
        "    if test_loss<best_loss:\n",
        "        best_loss=test_loss\n",
        "        best_model_weights=model.state_dict()\n",
        "        patience=10\n",
        "    else:\n",
        "        patience-=1\n",
        "        if patience==0:\n",
        "            break\n",
        "    scheduler.step()\n",
        "model.load_state_dict(best_model_weights)\n",
        "print(f'best loss:', best_loss)"
      ],
      "metadata": {
        "id": "yKjNHsY3jQsY",
        "outputId": "a76a93ea-3209-45cf-ab42-fc44d6ac2b25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.404695 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.298038 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9975e-04.\n",
            "Epoch: 2\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.314925 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.259149 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9901e-04.\n",
            "Epoch: 3\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.272572 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.309277 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9778e-04.\n",
            "Epoch: 4\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.208800 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.161119 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9606e-04.\n",
            "Epoch: 5\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.162049 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.160920 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9384e-04.\n",
            "Epoch: 6\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.124916 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.111742 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.9114e-04.\n",
            "Epoch: 7\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.106986 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.119696 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.8796e-04.\n",
            "Epoch: 8\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.088546 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.094396 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.8429e-04.\n",
            "Epoch: 9\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.118653 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.180996 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.8015e-04.\n",
            "Epoch: 10\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.069546 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.155258 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.7553e-04.\n",
            "Epoch: 11\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.054081 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.171861 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.7044e-04.\n",
            "Epoch: 12\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.050845 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.245391 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.6489e-04.\n",
            "Epoch: 13\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.050454 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.428536 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.5888e-04.\n",
            "Epoch: 14\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.045590 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.081969 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.5241e-04.\n",
            "Epoch: 15\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.039761 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.105443 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.4550e-04.\n",
            "Epoch: 16\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.032392 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.063309 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.3815e-04.\n",
            "Epoch: 17\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.031776 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.100587 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.3037e-04.\n",
            "Epoch: 18\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.033421 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.082904 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.2216e-04.\n",
            "Epoch: 19\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.031877 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.059717 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.1354e-04.\n",
            "Epoch: 20\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.029262 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.091700 \n",
            "\n",
            "Adjusting learning rate of group 0 to 9.0451e-04.\n",
            "Epoch: 21\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.011946 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.072038 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.9508e-04.\n",
            "Epoch: 22\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.011842 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.062497 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.8526e-04.\n",
            "Epoch: 23\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.012580 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.128936 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.7506e-04.\n",
            "Epoch: 24\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.009201 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.080376 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.6448e-04.\n",
            "Epoch: 25\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.012081 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.101704 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.5355e-04.\n",
            "Epoch: 26\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.008265 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.058052 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.4227e-04.\n",
            "Epoch: 27\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.005218 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.069532 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.3066e-04.\n",
            "Epoch: 28\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.004643 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.111058 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.1871e-04.\n",
            "Epoch: 29\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.003931 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.054672 \n",
            "\n",
            "Adjusting learning rate of group 0 to 8.0645e-04.\n",
            "Epoch: 30\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.005376 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.125259 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.9389e-04.\n",
            "Epoch: 31\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.007447 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.079321 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.8104e-04.\n",
            "Epoch: 32\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.026776 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.078108 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.6791e-04.\n",
            "Epoch: 33\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.021715 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.090288 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.5452e-04.\n",
            "Epoch: 34\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.004815 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.060304 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.4088e-04.\n",
            "Epoch: 35\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.001354 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.062994 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.2700e-04.\n",
            "Epoch: 36\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.000574 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.060760 \n",
            "\n",
            "Adjusting learning rate of group 0 to 7.1289e-04.\n",
            "Epoch: 37\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.000396 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.061240 \n",
            "\n",
            "Adjusting learning rate of group 0 to 6.9857e-04.\n",
            "Epoch: 38\n",
            "\n",
            "Train Error: \n",
            " Avg loss: 0.000314 \n",
            "\n",
            "Test Error: \n",
            " Avg loss: 0.061750 \n",
            "\n",
            "Adjusting learning rate of group 0 to 6.8406e-04.\n",
            "Epoch: 39\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_preds=[]\n",
        "with torch.no_grad():\n",
        "    for x,y in val_loader:\n",
        "        pred=model(x).squeeze(-1)\n",
        "        val_preds.append(pred.cpu().numpy())\n",
        "val_preds=np.concatenate(val_preds,axis=0)\n",
        "val_preds=np.round(val_preds)\n",
        "recall_score(y_valid.cpu().numpy(),val_preds),accuracy_score(y_valid.cpu().numpy(),val_preds)"
      ],
      "metadata": {
        "id": "0q97t12ljVqi",
        "outputId": "c15ab1f5-92ab-4400-8442-b5412423db7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-18e86b805233>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_preds=[]\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        pred=model(x).squeeze(-1)\n",
        "        test_preds.append(pred.cpu().numpy())\n",
        "test_preds=np.concatenate(test_preds,axis=0)\n",
        "test_preds=np.round(test_preds)\n",
        "recall_score(y_test.cpu().numpy(),test_preds),accuracy_score(y_test.cpu().numpy(),test_preds)"
      ],
      "metadata": {
        "id": "ZocXWWn_jazQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_parametrics(name,y_val, y_pred):\n",
        "\n",
        "    print(\"\\n------------------------{}------------------------\\n\".format(name))\n",
        "\n",
        "    cm_test = confusion_matrix(y_val, y_pred)\n",
        "    t1 = ConfusionMatrixDisplay(cm_test)\n",
        "    print(\"\\nClassification Report\\n\")\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(\"--------------------------------------------------------------------------\")\n",
        "\n",
        "    t1.plot()\n",
        "evaluation_parametrics(\"Confusion Matrix - Validation Dataset\", y_valid.cpu().numpy(), val_preds)"
      ],
      "metadata": {
        "id": "JP_rVVqOjdw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQOhOpvVjk_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}